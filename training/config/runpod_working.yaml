# WordPress SLM Training Configuration - TESTED WORKING ON RUNPOD
# This configuration has been validated to work on RunPod with RTX 4090
# Last tested: 2025-01-25 with TinyLlama-1.1B-Chat-v1.0

base_model: TinyLlama/TinyLlama-1.1B-Chat-v1.0
output_dir: /workspace/outputs/test-model

training:
  # Basic training parameters
  num_train_epochs: 1
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 4
  
  # CRITICAL: Gradient checkpointing must be disabled for LoRA compatibility
  gradient_checkpointing: false
  
  # Learning rate - MUST use decimal notation, not scientific (5e-5 causes YAML parsing issues)
  learning_rate: 0.00005
  lr_scheduler_type: linear
  warmup_ratio: 0.1
  
  # Optimizer settings
  optim: adamw_torch
  weight_decay: 0.01
  max_grad_norm: 1.0
  
  # Logging and evaluation - CRITICAL: Use 'eval_strategy' not 'evaluation_strategy'
  logging_steps: 10
  eval_steps: 50
  save_steps: 50
  save_strategy: steps
  eval_strategy: steps
  
  # Model management
  save_total_limit: 2
  load_best_model_at_end: true
  metric_for_best_model: eval_loss
  greater_is_better: false
  
  # Precision settings - CRITICAL: All disabled for LoRA compatibility
  bf16: false
  fp16: false
  tf32: false
  
  # Other settings
  seed: 42
  report_to: none
  push_to_hub: false
  max_seq_length: 512

# LoRA configuration - optimized for RunPod environment
lora:
  r: 8
  alpha: 16
  target_modules: [q_proj, v_proj, k_proj, o_proj, gate_proj, up_proj, down_proj]
  dropout: 0.05
  bias: none

# Expected results with this config:
# - Trainable parameters: ~6.3M (0.57% of total)
# - Training loss: ~0.79
# - Evaluation loss: ~0.76
# - Training time: ~5-10 minutes on RTX 4090