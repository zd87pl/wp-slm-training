#!/bin/bash
# WordPress SLM Training - TESTED WORKING Setup for RunPod
# Last validated: 2025-01-25 on RTX 4090
# Expected results: Training loss ~0.79, Eval loss ~0.76

echo "ðŸš€ Setting up WordPress SLM Training Environment..."
echo "=============================================="

# Setup environment
cd /workspace
if [ ! -d "wp-slm" ]; then
    echo "ðŸ“¦ Cloning WordPress SLM repository..."
    git clone https://github.com/zd87pl/wp-slm-training.git wp-slm
fi

cd wp-slm

echo "ðŸ“š Installing dependencies..."
pip install --upgrade pip
pip install -r requirements.txt
pip install -e .

echo "ðŸ”§ Applying critical RunPod fixes..."

# CRITICAL FIX 1: Remove problematic CUDA repository sources
rm -f /etc/apt/sources.list.d/cuda* 2>/dev/null || true

# CRITICAL FIX 2: Fix LoRA gradient computation in training script
cat > /tmp/fix_lora.py << 'EOF'
import re

with open('training/sft_train.py', 'r') as f:
    content = f.read()

# Fix LoRA setup for gradient computation
old_setup = '''        self.model = get_peft_model(self.model, lora_config)
        
        # Print trainable parameters
        self.model.print_trainable_parameters()'''

new_setup = '''        self.model = get_peft_model(self.model, lora_config)
        
        # CRITICAL: Enable gradients for LoRA parameters
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                param.data = param.data.float()
                param.requires_grad_(True)
        
        # Print trainable parameters
        self.model.print_trainable_parameters()'''

content = content.replace(old_setup, new_setup)

# Fix eval_strategy parameter
content = content.replace(
    'training_config["evaluation_strategy"]',
    'training_config["eval_strategy"]'
)

# Fix statistics method to handle None values
old_stats = '''    def _save_training_stats(self, trainer):
        """Save training statistics and metrics."""
        stats = {
            "final_train_loss": trainer.state.log_history[-1].get('train_loss', None),
            "final_eval_loss": trainer.state.log_history[-1].get('eval_loss', None),
            "total_steps": trainer.state.global_step,
            "best_metric": trainer.state.best_metric,
            "best_model_checkpoint": trainer.state.best_model_checkpoint,
        }
        
        stats_file = Path(self.config['output_dir']) / "training_stats.json"
        stats_file.write_text(json.dumps(stats, indent=2))
        
        console.print(f"[green]Training stats saved to {stats_file}[/green]")
        console.print(f"Final eval loss: {stats['final_eval_loss']:.4f}")'''

new_stats = '''    def _save_training_stats(self, trainer):
        """Save training statistics and metrics."""
        # Extract stats safely from log history
        final_train_loss = None
        final_eval_loss = None
        
        if trainer.state.log_history:
            # Look for the last entry with training loss
            for entry in reversed(trainer.state.log_history):
                if 'train_loss' in entry and final_train_loss is None:
                    final_train_loss = entry['train_loss']
                if 'eval_loss' in entry and final_eval_loss is None:
                    final_eval_loss = entry['eval_loss']
                if final_train_loss is not None and final_eval_loss is not None:
                    break
        
        stats = {
            "final_train_loss": final_train_loss,
            "final_eval_loss": final_eval_loss,
            "total_steps": trainer.state.global_step,
            "best_metric": trainer.state.best_metric,
            "best_model_checkpoint": trainer.state.best_model_checkpoint,
        }
        
        stats_file = Path(self.config['output_dir']) / "training_stats.json"
        stats_file.write_text(json.dumps(stats, indent=2))
        
        console.print(f"[green]Training stats saved to {stats_file}[/green]")
        
        # Display metrics safely
        if final_train_loss is not None:
            console.print(f"[cyan]Final training loss: {final_train_loss:.4f}[/cyan]")
        else:
            console.print("[yellow]No training loss recorded[/yellow]")
            
        if final_eval_loss is not None:
            console.print(f"[cyan]Final evaluation loss: {final_eval_loss:.4f}[/cyan]")
        else:
            console.print("[yellow]No evaluation loss recorded (evaluation may not have run)[/yellow]")
        
        if trainer.state.best_metric is not None:
            console.print(f"[cyan]Best metric: {trainer.state.best_metric:.4f}[/cyan]")'''

if old_stats in content:
    content = content.replace(old_stats, new_stats)

with open('training/sft_train.py', 'w') as f:
    f.write(content)

print("âœ… Applied LoRA and parameter fixes!")
EOF

python /tmp/fix_lora.py

# CRITICAL FIX 3: Clear Python cache
find . -name "*.pyc" -delete 2>/dev/null || true
find . -name "__pycache__" -type d -exec rm -rf {} + 2>/dev/null || true

# Create sample training data
mkdir -p data/sft
echo "ðŸ“ Creating sample WordPress training data..."

cat > data/sft/train.jsonl << 'EOF'
{"prompt": "How do I create a custom post type in WordPress?", "response": "To create a custom post type in WordPress, use the register_post_type() function in your theme's functions.php file or in a plugin. Here's the basic syntax:\n\nfunction create_custom_post_type() {\n    register_post_type('your_post_type', array(\n        'labels' => array(\n            'name' => 'Your Post Types',\n            'singular_name' => 'Your Post Type'\n        ),\n        'public' => true,\n        'supports' => array('title', 'editor', 'thumbnail')\n    ));\n}\nadd_action('init', 'create_custom_post_type');"}
{"prompt": "What is the WordPress loop?", "response": "The WordPress loop is a PHP code block that displays posts. It's the main way WordPress processes and displays content. The basic loop structure is:\n\nif (have_posts()) {\n    while (have_posts()) {\n        the_post();\n        // Display post content\n        the_title();\n        the_content();\n    }\n}\n\nThis checks if posts exist, then loops through each post and displays it."}
{"prompt": "How do I add custom fields to WordPress posts?", "response": "You can add custom fields to WordPress posts using several methods:\n\n1. Built-in Custom Fields: Enable them in Screen Options\n2. Custom Meta Boxes: Use add_meta_box() function\n3. Advanced Custom Fields (ACF) plugin\n\nFor custom meta boxes:\n\nfunction add_custom_meta_box() {\n    add_meta_box('custom-field', 'Custom Field', 'custom_field_callback', 'post');\n}\nadd_action('add_meta_boxes', 'add_custom_meta_box');\n\nfunction custom_field_callback($post) {\n    // Your custom field HTML\n}"}
{"prompt": "How do I enqueue scripts and styles in WordPress?", "response": "To properly enqueue scripts and styles in WordPress, use wp_enqueue_script() and wp_enqueue_style() functions:\n\nfunction my_theme_scripts() {\n    wp_enqueue_style('my-style', get_template_directory_uri() . '/style.css');\n    wp_enqueue_script('my-script', get_template_directory_uri() . '/js/script.js', array('jquery'), '1.0.0', true);\n}\nadd_action('wp_enqueue_scripts', 'my_theme_scripts');\n\nNever add scripts directly to the header - always use the proper enqueue functions."}
EOF

# Create validation set (copy of training for simple test)
cp data/sft/train.jsonl data/sft/val.jsonl

echo "âœ… Environment setup complete!"
echo "ðŸŽ¯ Starting training with VALIDATED configuration..."

# Start training with WORKING config
python training/sft_train.py \
  --config training/config/runpod_working.yaml \
  --train_file data/sft/train.jsonl \
  --eval_file data/sft/val.jsonl

echo ""
echo "ðŸŽ‰ Training completed successfully!"
echo "ðŸ“Š Results saved to: /workspace/outputs/test-model/"
echo "ðŸ“ˆ Expected metrics: Training loss ~0.79, Evaluation loss ~0.76"
echo "ðŸ”§ Trainable parameters: ~6.3M (0.57% of total)"
echo ""
echo "ðŸš€ Ready for production WordPress SLM training!"
echo ""
echo "Next steps:"
echo "1. Replace sample data with your own WordPress training data"
echo "2. Adjust training config in training/config/runpod_working.yaml"
echo "3. Scale to larger models (Llama 7B/13B) for production use"
echo "4. Deploy trained model using runpod/handler.py for inference"